---
layout: docs
page_title: Garbage collection
description: |-
  Learn how Nomad implements garbage collection.
---

# Garbage collection

this page how gc works. may need to add second page under operations for how to
tune garbage collection.

garbage collection
- nodes
- jobs
- allocations (instances of running jobs)
- evaluations
- deployments
- encryption keys

---

The existing documentation lists the various configurations associated with GC, but as this is spread across both server and client configuration, there is no singular place to learn about how GC is implemented by Nomad & what are the available ways to tune it.

Some questions that I think would be useful for such a document to answer are:

- **What are the events that explicitly trigger a GC run?** For example, for
  allocations, the code suggests that the only triggers for GC are (1) the
  gc_interval elapsing, or (2) allocations being created/terminated, or (3)
  server-side removals triggered by API/CLI calls (nomad system gc) or GCs of
  evaluations (see cascading bullet below). The existing documentation IMO could
  be misinterpreted to mean that GC runs are also triggered by the disk/inode
  thresholds being surpassed (e.g. as if the Nomad client watching/polling its
  host's disk usage continuously), which is not the case. Trigger (2) is also
  not mentioned in any of the docs, which can lead a reader to mistakenly
  believe alloc GCs are only triggered at gc_interval elapsing.
- **Once an allocation GC is triggered, how many allocations will be destroyed and
  how are they chosen?** The code suggests that for triggers (1) and (2) above,
  allocations are removed in termination-time order until no
  disk/inode/max-alloc thresholds are surpassed, and that only in case (3) are
  allocations destroyed if none of these thresholds are surpassed. I wasn't able
  to find this information from the docs alone.
- **What are the triggers for non-allocation GC runs?**
- **What are available configurations for how non-alloc resources are chosen for
  GC?** This is obvious from reading server stanza documentation; but what's not
  immediately obvious, without reading through all of client gc_* and server
  *_gc_* stanza configurations, is that server configs are only for non-alloc
  resources (except on cascading GC's -- see below), and client configs are only
  for alloc resources. Similarly, not all resources have the same available
  controls -- e.g. allocations do not have something like an alloc_gc_threshold
  configuration similar to (job|deployment|eval)_gc_threshold. Having one place
  that lists out the GC'able resources and their associated controls across
  client & server config would be useful.
- **What sorts of cascading GC (if any) exist?** For example, if a job resource
  is GC'd, does that include all of its deployments/evals/allocations as well?
  Or, if an eval is GC'd on the server, are the terminal allocations also GC'd?
  This latter case appears to be true, but I wasn't able to find it mentioned in
  the existing docs (save for [this comment
  block](https://pkg.go.dev/github.com/hashicorp/nomad@v1.3.0/nomad/structs#CoreJobEvalGC),
  and as this effectively makes the server's eval_gc_threshold config an
  implicit age threshold on terminal allocations, it'd be useful to document
  this so that it can be tuned accordingly alongside the client-side alloc
  parameters.

I understand that there may be things that committing to documentation would
make future optimizations more difficult (e.g. not committing an order of alloc
termination so that a future implementation could, for example, destroy allocs
based on disk usage if that's the threshold the client's trying to drop below).
I think it'd be reasonable to leave out anything that there isn't a desire to
commit to in the docs, and maybe to call out that anything not described
explicitly is subject to change.

Attempted Solutions

 It's possible to glean a full picture by searching for gc
across the client & server stanza configuration docs, and by reading

- [client/gc.go](https://github.com/hashicorp/nomad/blob/9347613/client/gc.go)
  for the client-side alloc GC algorithm
- [client/client.go](https://github.com/hashicorp/nomad/blob/9347613/client/client.go)  for the various non-timer GC triggers
- server-side code (e.g.
[nomad/core_sched.go](https://github.com/hashicorp/nomad/blob/9347613/nomad/core_sched.go)) for other cases that can trigger GC

but this is not ideal & toilsome to share with non-Go-developer audiences within
an organization that have a stake in GC configuration.

(Apologies in advance if this documentation exists in other forms and I wasn't able to find it; if that's the case, I propose linking to those docs from the client/server config docs.)

-----
 Clients have different GC parameters, e.g. gc_disk_usage_threshold,
 gc_max_allocs. If the alloc terminates, and free space is low and/or a lot of
 allocations are running on the client, then the client will GC the allocation.
 I expect the job info to be still present on server info, but without the
 stats, logs, and file system info.

Worth noting that Nomad only GC jobs that are dead and aren't expected to restart again (without manual intervention). If a job lasts for days (e.g. service jobs, super long batch job), it will not be GCed. Only after the job completes or stopped manually, will Nomad GC it. jobIsGCable function clarifies which jobs are GCable, and jobGC performs the threshold check. Also, to clarify, GC is mostly meant as an internal process that prevents Nomad from ever growing memory usage - its only user visible affect should be that completed jobs eventually get removed from API results.

## Client

Settings:
	MaxAllocs           int
	DiskUsageThreshold  float64
	InodeUsageThreshold float64
	Interval            time.Duration
	ReservedDiskMB      int
	ParallelDestroys    int

## Evaluation

// CoreJobEvalGC is used for the garbage collection of evaluations
	// and allocations. We periodically scan evaluations in a terminal state,
	// in which all the corresponding allocations are also terminal. We
	// delete these out of the system to bound the state.
	CoreJobEvalGC = "eval-gc"

# Allocation
 // CoreJobEvalGC is used for the garbage collection of evaluations
	// and allocations. We periodically scan evaluations in a terminal state,
	// in which all the corresponding allocations are also terminal. We
	// delete these out of the system to bound the stat
 If the alloc terminates, and free space is low and/or a lot of allocations are
 running on the client, then the client will GC the allocation.

AllocGarbageCollector garbage collects terminated allocations on a node

destroyAllocRunner is used to destroy an allocation runner. It will acquire a
lock to restrict parallelism and then destroy the alloc runner, returning
once the allocation has been destroyed.

	// If the host has enough free space to accommodate the new allocations then
	// we don't need to garbage collect terminated allocations

## Node
	// CoreJobNodeGC is used for the garbage collection of failed nodes.
	// We periodically scan nodes in a terminal state, and if they have no
	// corresponding allocations we delete these out of the system.
	CoreJobNodeGC = "node-gc"

## Job
	// CoreJobJobGC is used for the garbage collection of eligible jobs. We
	// periodically scan garbage collectible jobs and check if both their
	// evaluations and allocations are terminal. If so, we delete these out of
	// the system.
	CoreJobJobGC = "job-gc"

## Deployment
	// CoreJobDeploymentGC is used for the garbage collection of eligible
	// deployments. We periodically scan garbage collectible deployments and
	// check if they are terminal. If so, we delete these out of the system.
	CoreJobDeploymentGC = "deployment-gc"

## CSI objects

### volume claim
	// CoreJobCSIVolumeClaimGC is use for the garbage collection of CSI
	// volume claims. We periodically scan volumes to see if no allocs are
	// claiming them. If so, we unclaim the volume.
	CoreJobCSIVolumeClaimGC = "csi-volume-claim-gc"

### plugins
	// CoreJobCSIPluginGC is use for the garbage collection of CSI plugins.
	// We periodically scan plugins to see if they have no associated volumes
	// or allocs running them. If so, we delete the plugin.
	CoreJobCSIPluginGC = "csi-plugin-gc"

## Tokens
### One-time tokens
	// CoreJobOneTimeTokenGC is use for the garbage collection of one-time
	// tokens. We periodically scan for expired tokens and delete them.
	CoreJobOneTimeTokenGC = "one-time-token-gc"

### Local ACL tokens
	// CoreJobLocalTokenExpiredGC is used for the garbage collection of
	// expired local ACL tokens. We periodically scan for expired tokens and
	// delete them.
	CoreJobLocalTokenExpiredGC = "local-token-expired-gc"

### Global ACL tokens
	// CoreJobGlobalTokenExpiredGC is used for the garbage collection of
	// expired global ACL tokens. We periodically scan for expired tokens and
	// delete them.
	CoreJobGlobalTokenExpiredGC = "global-token-expired-gc"

## Keys
	// CoreJobRootKeyRotateGC is used for periodic key rotation and
	// garbage collection of unused encryption keys.
	CoreJobRootKeyRotateOrGC = "root-key-rotate-gc"

	// CoreJobVariablesRekey is used to fully rotate the encryption keys for
	// variables by decrypting all variables and re-encrypting them with the
	// active key
	CoreJobVariablesRekey = "variables-rekey"

	// CoreJobForceGC is used to force garbage collection of all GCable objects.
	CoreJobForceGC = "force-gc"


## configuration

### client
https://developer.hashicorp.com/nomad/docs/configuration/client

```
client {
  gc_interval (string: "1m") - Specifies the interval at which Nomad attempts to garbage collect terminal allocation directories.

gc_disk_usage_threshold (float: 80) - Specifies the disk usage percent which Nomad tries to maintain by garbage collecting terminal allocations.

gc_inode_usage_threshold (float: 70) - Specifies the inode usage percent which Nomad tries to maintain by garbage collecting terminal allocations.

gc_max_allocs (int: 50) - Specifies the maximum number of allocations which a client will track before triggering a garbage collection of terminal allocations. This will not limit the number of allocations a node can run at a time, however after gc_max_allocs every new allocation will cause terminal allocations to be GC'd.

gc_parallel_destroys (int: 2) - Specifies the maximum number of parallel destroys allowed by the garbage collector. This value should be relatively low to avoid high resource usage during garbage collections.
}

### server

```
server {

}
```

https://developer.hashicorp.com/nomad/docs/configuration/server

node_gc_threshold (string: "24h") - Specifies how long a node must be in a terminal state before it is garbage collected and purged from the system. This is specified using a label suffix like "30s" or "1h".

job_gc_interval (string: "5m") - Specifies the interval between the job garbage collections. Only jobs who have been terminal for at least job_gc_threshold will be collected. Lowering the interval will perform more frequent but smaller collections. Raising the interval will perform collections less frequently but collect more jobs at a time. Reducing this interval is useful if there is a large throughput of tasks, leading to a large set of dead jobs. This is specified using a label suffix like "30s" or "3m".

job_gc_threshold (string: "4h") - Specifies the minimum time a job must be in the terminal state before it is eligible for garbage collection. This is specified using a label suffix like "30s" or "1h".

eval_gc_threshold (string: "1h") - Specifies the minimum time an evaluation must be in the terminal state before it is eligible for garbage collection. This is specified using a label suffix like "30s" or "1h". Note that batch job evaluations are controlled via batch_eval_gc_threshold.

batch_eval_gc_threshold (string: "24h") - Specifies the minimum time an evaluation stemming from a batch job must be in the terminal state before it is eligible for garbage collection. This is specified using a label suffix like "30s" or "1h". Note that the threshold is a necessary but insufficient condition for collection, and the most recent evaluation won't be garbage collected even if it breaches the threshold.

deployment_gc_threshold (string: "1h") - Specifies the minimum time a deployment must be in the terminal state before it is eligible for garbage collection. This is specified using a label suffix like "30s" or "1h".

csi_volume_claim_gc_interval (string: "5m") - Specifies the interval between CSI volume claim garbage collections.

csi_volume_claim_gc_threshold (string: "1h") - Specifies the minimum age of a CSI volume before it is eligible to have its claims garbage collected. This is specified using a label suffix like "30s" or "1h".

csi_plugin_gc_threshold (string: "1h") - Specifies the minimum age of a CSI plugin before it is eligible for garbage collection if not in use. This is specified using a label suffix like "30s" or "1h".

acl_token_gc_threshold (string: "1h") - Specifies the minimum age of an expired ACL token before it is eligible for garbage collection. This is specified using a label suffix like "30s" or "1h".


root_key_gc_interval (string: "10m") - Specifies the interval between encryption key metadata garbage collections.

root_key_gc_threshold (string: "1h") - Specifies the minimum time after the root_key_rotation_threshold has passed that an encryption key must exist before it can be eligible for garbage collection.
